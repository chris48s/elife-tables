<table-wrap xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" id="table1" position="float"><object-id pub-id-type="doi">10.7554/eLife.44838.010</object-id><label>Table 1.</label><caption><title>Comparison of different models fitted to the animals&#8217; choices.</title><p>Best fitting model indicated in bold.</p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" valign="top">Model</th><th rowspan="2" valign="top">Description</th><th colspan="2" valign="top">Both animals</th><th colspan="2" valign="top">Animal A</th><th colspan="2" valign="top">Animal B</th></tr><tr><th valign="top">AIC</th><th valign="top">BIC</th><th valign="top">AIC</th><th valign="top">BIC</th><th valign="top">AIC</th><th valign="top">BIC</th></tr></thead><tbody><tr><td valign="top">(1)</td><td valign="top">Value from reward history<sup>1</sup></td><td valign="top">2.2482</td><td valign="top">2.2490</td><td valign="top">1.5077</td><td valign="top">1.5084</td><td valign="top">7.3571</td><td valign="top">7.3636</td></tr><tr><td valign="top">(2)</td><td valign="top">Value from reward history and risk<sup>2</sup></td><td valign="top">2.2477</td><td valign="top">2.2492</td><td valign="top">1.5077</td><td valign="top">1.5092</td><td valign="top">7.3522</td><td valign="top">7.3653</td></tr><tr><td valign="top">(3)</td><td valign="top">Value from choice history<sup>3</sup></td><td valign="top">2.1614</td><td valign="top">2.1622</td><td valign="top">1.4900</td><td valign="top">1.4907</td><td valign="top">6.5043</td><td valign="top">6.5109</td></tr><tr><td valign="top">(4)</td><td valign="top">Value from choice history and risk</td><td valign="top">2.0385</td><td valign="top">2.0400</td><td valign="top">1.4023</td><td valign="top">1.4037</td><td valign="top">7.3528</td><td valign="top">7.3660</td></tr><tr><td valign="top">(5)</td><td valign="top">Value from reward and choice history<sup>4</sup></td><td valign="top">2.0089</td><td valign="top">2.0097</td><td valign="top">1.3914</td><td valign="top">1.3922</td><td valign="top">6.0880</td><td valign="top">6.0945</td></tr><tr><td valign="top"><bold>(6)</bold></td><td valign="top"><bold>Value from reward and choice history and risk</bold></td><td valign="top"><bold>2.0073</bold></td><td valign="top"><bold>2.0088</bold></td><td valign="top"><bold>1.3899</bold></td><td valign="top"><bold>1.3914</bold></td><td valign="top"><bold>6.0747</bold></td><td valign="top"><bold>6.0878</bold></td></tr><tr><td valign="top">(7)</td><td valign="top">Objective reward probabilities<sup>5</sup></td><td valign="top">2.1213</td><td valign="top">2.1220</td><td valign="top">1.4615</td><td valign="top">1.4622</td><td valign="top">6.4972</td><td valign="top">6.5037</td></tr><tr><td valign="top">(8)</td><td valign="top">Objective reward probabilities and objective risk<sup>6</sup></td><td valign="top">2.1210</td><td valign="top">2.1225</td><td valign="top">1.4616</td><td valign="top">1.4631</td><td valign="top">6.4982</td><td valign="top">6.5114</td></tr><tr><td valign="top">(9)</td><td valign="top">Reinforcement learning (RL) model<sup>7</sup></td><td valign="top">2.0763</td><td valign="top">2.0779</td><td valign="top">1.4376</td><td valign="top">1.4391</td><td valign="top">6.2161</td><td valign="top">6.2293</td></tr><tr><td valign="top">(10)</td><td valign="top">RL learning, stack parameter (<xref ref-type="bibr" rid="bib25">Huh et al., 2009</xref>)<sup>8</sup></td><td valign="top">2.0810</td><td valign="top">2.0826</td><td valign="top">1.4374</td><td valign="top">1.4389</td><td valign="top">6.3198</td><td valign="top">6.3330</td></tr><tr><td valign="top">(11)</td><td valign="top">RL, reversal-learning variant<sup>9</sup></td><td valign="top">2.2614</td><td valign="top">2.2630</td><td valign="top">1.5330</td><td valign="top">1.5344</td><td valign="top">7.2808</td><td valign="top">7.2939</td></tr></tbody></table><table-wrap-foot><fn><p>1:Value defined according to <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>; 2: Risk defined according to <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>; 3: Value defined as sum of weighted choice history derived from <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>; 4: Value defined according to <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>; 5: Objective reward probabilities defined according to <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>; 6: Objective reward risk defined according to <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>; 7: Standard Rescorla-Wagner RL model updating value of chosen option based on last outcome; 8: Modified RL model incorporating choice-dependency; 9: Modified RL model updating value of chosen and unchosen option based on last outcome.</p></fn></table-wrap-foot></table-wrap>