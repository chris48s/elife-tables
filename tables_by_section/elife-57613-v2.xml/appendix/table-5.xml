<table-wrap xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" id="app6table2" position="float"><label>Appendix 6&#8212;table 2.</label><caption><title>Parameters guide for CNN Predictions and Post-processing.</title><p>Menu B and D in <xref ref-type="fig" rid="app6fig1">Appendix 6&#8212;figure 1</xref>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Process type</th><th>Parameter name</th><th>Description</th><th>Range</th><th>Default</th></tr></thead><tbody><tr><td>CNN Prediction</td><td>Model Name</td><td>Trained model name. Models trained on confocal (model name: &#8216;generic_confocal_3D_unet&#8217;) and lightsheet (model name: &#8216;generic_confocal_3D_unet&#8217;) data as well as their multi-resolution variants are available: More info on available models and importing custom models can be found in the project repository.</td><td>text</td><td>&#8216;generic_confocal&#8230;&#8217;</td></tr><tr><td/><td>Patch Size</td><td>Patch size given to the network. A bigger patches cost more memory but can give a slight improvement in performance. For 2D segmentation the Patch size relative to the z axis has to be set to 1.</td><td>tuple</td><td><inline-formula><mml:math id="inf44"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>32</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo>,</mml:mo><mml:mn>128</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula></td></tr><tr><td/><td>Stride</td><td>Specifies the overlap between neighboring patches. The bigger the overlap the the better predictions at the cost of additional computation time. In the GUI the stride values are automatically set, the user can choose between: Accurate (50% overlap between patches), Balanced (25% overlap between patches), Draft (only 5% overlap between patches).</td><td>menu</td><td>Balanced</td></tr><tr><td/><td>Device Type</td><td>If a CUDA capable gpu is available and setup correctly, &#8216;cuda&#8217; should be used, otherwise one can use &#8216;cpu&#8217; for cpu only inference (much slower).</td><td>menu</td><td>&#8216;cpu&#8217;</td></tr><tr><td>Prediction Post-processing</td><td>Convert to tiff</td><td>If True the prediction is exported as tiff file.</td><td>bool</td><td>False</td></tr><tr><td/><td>Cast Predictions</td><td>Predictions stacks are generated in &#8216;float32&#8217;. Or &#8216;uint8&#8217; can be alternatively used to reduce the memory footprint.</td><td>menu</td><td>&#8216;data_float32&#8217;</td></tr></tbody></table></table-wrap>